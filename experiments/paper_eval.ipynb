{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook reproduces the evaluation results from the paper.\n",
    "\n",
    "Note: As of version 2.1b6.dev234, the Essentia library has a [bug](https://github.com/MTG/essentia/issues/1054) that causes an infinite loop for some inputs.\n",
    "To avoid this, you have to build our patched version of Essentia: https://github.com/cifkao/essentia/tree/patched\n",
    "\n",
    "Copyright 2020 InterDigital R&D and Télécom Paris.  \n",
    "Author: Ondřej Cífka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the outputs\n",
    "Before running the evaluation, we need to obtain the outputs of all the systems on both of our test sets and place them in the `outputs/synth` and `outputs/real` directories (for the artificial and real inputs, respectively). The commands are different for each system:\n",
    "\n",
    "### VQ-VAE\n",
    "```sh\n",
    "python -m ss_vq_vae.models.vqvae_oneshot --logdir=model run ../data/lmd/audio_test/pairs \\\n",
    "    outputs/synth/vqvae_list outputs/synth/vqvae\n",
    "python -m ss_vq_vae.models.vqvae_oneshot --logdir=model run ../data/mixing_secrests/test/pairs \\\n",
    "    outputs/real/vqvae_list outputs/real/vqvae\n",
    "```\n",
    "The first command runs the model on all audio file pairs listed in the `../data/lmd/audio_test/pairs` file, writes the output files to the `outputs/synth/vqvae` directory and their paths to the file `outputs/synth/vqvae_list`. The second command does the same for the other test set.\n",
    "\n",
    "### U+L (Ulyanov and Lebedev)\n",
    "```sh\n",
    "python -m ss_vq_vae.models.ulyanov --style-weight-log=-2.1 ../data/lmd/audio_test/pairs \\\n",
    "    outputs/synth/ulyanov_swopt_list outputs/synth/ulyanov\n",
    "python -m ss_vq_vae.models.ulyanov --style-weight-log=-2.1 ../data/mixing_secrets/test/pairs \\\n",
    "    outputs/real/ulyanov_swopt_list outputs/real/ulyanov\n",
    "```\n",
    "\n",
    "### Musaicing (Driedger et al.)\n",
    "Clone Chris Tralie's [LetItBee repo](https://github.com/ctralie/LetItBee) and run the `Musaicing.py` script on each pair of audio files according to the instructions. Specify the content file using the `--target` option and the style file using the `--source` option, e.g.:\n",
    "```sh\n",
    "python LetItBee/Musaicing.py --sr 16000 \\\n",
    "    --source ../data/lmd/audio_test/wav_16kHz/voices1_pitch1/00484d071147e49551de9ffb141e8b9e.style.wav \\\n",
    "    --target ../data/lmd/audio_test/wav_16kHz/voices1_pitch1/00484d071147e49551de9ffb141e8b9e.content.wav \\\n",
    "    --result outputs/synth/driedger/00000.wav\n",
    "```\n",
    "You might want to run these commands in parallel as they are time-consuming. Remember to write the list of output files to the `outputs/{synth,real}/driedger_list` file in the correct order, so that the evaluation code can pick them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import essentia.standard as estd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from ss_vq_vae.models import triplet_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000\n",
    "\n",
    "MFCC_KWARGS = dict(\n",
    "    n_mfcc=13,\n",
    "    hop_length=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"backbone\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 12)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          3136      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 64)          8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 45,312\n",
      "Trainable params: 44,800\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Model: \"triplet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "backbone (Functional)           (None, 64)           45312       anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           backbone[0][0]                   \n",
      "                                                                 backbone[1][0]                   \n",
      "                                                                 backbone[0][0]                   \n",
      "                                                                 backbone[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2, 1)         0           lambda[0][0]                     \n",
      "                                                                 lambda[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 45,312\n",
      "Trainable params: 44,800\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcd207cba90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_model, triplet_backbone = triplet_network.build_model(num_features=12)\n",
    "triplet_model.load_weights('timbre_metric/checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paths(tsv_path, column_names):\n",
    "    parent_dir = os.path.dirname(tsv_path)\n",
    "    df = pd.read_csv(tsv_path, sep='\\t', names=column_names)\n",
    "    df = df.applymap(lambda x: os.path.join(parent_dir, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_power(audio):\n",
    "    return audio / (np.sqrt(np.mean(audio ** 2)) + np.finfo(audio.dtype).eps)\n",
    "\n",
    "def get_pitches(audio):\n",
    "    input_sr, sr = SR, 8000  # Need to resample because of EqualLoudness\n",
    "    audio = estd.Resample(inputSampleRate=input_sr, outputSampleRate=sr)(audio)\n",
    "    audio = estd.EqualLoudness(sampleRate=sr)(audio)\n",
    "    rng = np.random.default_rng(seed=(audio > 0).sum())\n",
    "    audio = rng.normal(loc=audio, scale=1e-4).astype(audio.dtype)  # To prevent Melodia from crashing\n",
    "    pitches = estd.MultiPitchMelodia(sampleRate=sr)(audio)\n",
    "    pitches = [[pretty_midi.utilities.hz_to_note_number(p) for p in pl if not np.isclose(0, p)]\n",
    "               for pl in pitches]\n",
    "    pitches = [[int(p + 0.5) for p in pl] for pl in pitches]\n",
    "    return pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_example_target(output, reference):\n",
    "    def spec(audio):\n",
    "        audio = normalize_power(audio)\n",
    "        s = librosa.feature.melspectrogram(audio, sr=SR)\n",
    "        return librosa.power_to_db(s)\n",
    "    \n",
    "    s_out, s_ref = spec(output), spec(reference)\n",
    "    lsd = np.mean(np.sqrt(np.mean((s_out - s_ref) ** 2, axis=1)))\n",
    "\n",
    "    return {'lsd': lsd}\n",
    "\n",
    "def eval_example_style(output, reference):\n",
    "    mfcc_out = librosa.feature.mfcc(output, sr=SR, **MFCC_KWARGS)[1:]\n",
    "    mfcc_ref = librosa.feature.mfcc(reference, sr=SR, **MFCC_KWARGS)[1:]\n",
    "    \n",
    "    mfcc_triplet_cos, _ = 1 - triplet_model.predict([\n",
    "        (mfcc_ref.T[None, :, :], mfcc_out.T[None, :, :], mfcc_out.T[None, :, :])]).reshape(2)\n",
    "\n",
    "    return {'mfcc_triplet_cos': mfcc_triplet_cos}\n",
    "\n",
    "def eval_example_content(output, reference):\n",
    "    pitches_output, pitches_reference = get_pitches(output), get_pitches(reference)\n",
    "    assert len(pitches_output) == len(pitches_reference)\n",
    "    jaccard = []\n",
    "    for pl_output, pl_reference in zip(pitches_output, pitches_reference):\n",
    "        matches = len(set(pl_output) & set(pl_reference))\n",
    "        total = len(set(pl_output) | set(pl_reference))\n",
    "        if total == 0:\n",
    "            jaccard.append(0)\n",
    "        else:\n",
    "            jaccard.append(1 - matches / total)\n",
    "    jaccard = np.mean(jaccard)\n",
    "    return {'pitch_jaccard': jaccard}\n",
    "\n",
    "def pad_or_truncate(audio, reference):\n",
    "    if len(audio) < len(reference):\n",
    "        return np.pad(audio, (0, max(0, len(reference) - len(audio))))\n",
    "    return audio[:len(reference)]\n",
    "\n",
    "def eval_row_synth(row):\n",
    "    audio = row.apply(lambda path: librosa.load(path, sr=SR)[0])\n",
    "    audio = audio.apply(pad_or_truncate, reference=audio['target'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        key: {\n",
    "            **eval_example_target(audio[key], audio['target']),\n",
    "            **eval_example_style(audio[key], audio['target']),\n",
    "            **eval_example_content(audio[key], audio['target'])\n",
    "        }\n",
    "        for key in row.keys() if key != 'target'\n",
    "    }).stack()\n",
    "        \n",
    "def eval_row_real(row):\n",
    "    audio = row.apply(lambda path: librosa.load(path, sr=SR)[0])\n",
    "    audio_ref = audio[['content', 'style']]\n",
    "    audio = audio.apply(pad_or_truncate, reference=audio_ref['content'])\n",
    "    return pd.DataFrame({\n",
    "        key: {\n",
    "            **eval_example_style(audio[key], audio_ref['style']),\n",
    "            **eval_example_content(audio[key], audio_ref['content'])\n",
    "        }\n",
    "        for key in row.keys()\n",
    "    }).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_synth_df = pd.concat([\n",
    "    read_paths('../data/lmd/audio_test/triplets',\n",
    "               ['content', 'style', 'target']),\n",
    "    read_paths('outputs/synth/vq-vae_list',\n",
    "               ['vq-vae']),\n",
    "    read_paths('outputs/synth/driedger_list',\n",
    "               ['driedger']),\n",
    "    read_paths('outputs/synth/ulyanov_list',\n",
    "               ['ulyanov']),\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_real_df = pd.concat([\n",
    "    read_paths('../data/mixing_secrets/test/pairs',\n",
    "               ['content', 'style']),\n",
    "    read_paths('outputs/real/vq-vae_list',\n",
    "               ['vq-vae']),\n",
    "    read_paths('outputs/real/driedger_list',\n",
    "               ['driedger']),\n",
    "    read_paths('outputs/real/ulyanov_list',\n",
    "               ['ulyanov'])\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(paths_synth_df)) as pbar:\n",
    "    pbar.update(-1)\n",
    "    def fn(x):\n",
    "        y = eval_row_synth(x)\n",
    "        pbar.update(1)\n",
    "        return y\n",
    "    results_synth = paths_synth_df.apply(fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(paths_real_df)) as pbar:\n",
    "    pbar.update(-1)\n",
    "    def fn(x):\n",
    "        y = eval_row_real(x)\n",
    "        pbar.update(1)\n",
    "        return y\n",
    "    results_real = paths_real_df.apply(fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_synth.to_pickle('results_synth.pickle')\n",
    "results_real.to_pickle('results_real.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_synth = pd.read_pickle('results_synth.pickle')\n",
    "results_real = pd.read_pickle('results_real.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.concat([results_synth, results_real], axis=1, keys=['synth', 'real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_all.mean()).unstack(level=0).unstack(level=0).droplevel(axis=1, level=0).drop(('real', 'lsd'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{3}{l}{synth} & \\multicolumn{2}{l}{real} \\\\\n",
      "{} & lsd & mfcc\\_triplet\\_cos & pitch\\_jaccard & mfcc\\_triplet\\_cos & pitch\\_jaccard \\\\\n",
      "\\midrule\n",
      "content & 14.62 & 0.3713 & 0.5365 & 0.4957 & 0.0000 \\\\\n",
      "style & 20.36 & 0.2681 & 0.8729 & 0.0000 & 0.9099 \\\\\n",
      "ulyanov & 14.50 & 0.3483 & 0.5441 & 0.4792 & 0.1315 \\\\\n",
      "driedger & 14.51 & 0.2933 & 0.6445 & 0.2319 & 0.6297 \\\\\n",
      "vq-vae & 12.16 & 0.2063 & 0.5500 & 0.2278 & 0.6197 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex = (pd.DataFrame(results_all.mean())\n",
    "      .unstack(level=0).unstack(level=0)\n",
    "      .droplevel(axis=1, level=0)\n",
    "      .drop(('real', 'lsd'), axis=1)\n",
    "      .loc[['content', 'style', 'ulyanov', 'driedger', 'vq-vae']]\n",
    "      .to_latex(formatters=[x.format for x in ['{:0.2f}', '{:0.4f}', '{:0.4f}', '{:0.4f}', '{:0.4f}']]))\n",
    "latex = re.sub(r' +', ' ', latex)\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
